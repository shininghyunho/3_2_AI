Q.뉴런,퍼셉트론이란
A.노드=뉴런=결정 함수, 퍼셉트론=노들들의 구조

Q.웨이트의 역할
A.정보를 학습,기억하는 역할

Q.single layer 한계
A.선형 모형만 해결 가능

Q.비선형 문제 해결 방법
A.직선의 갯수 늘림, 입력 백터의 차원을 높임

Q.2nd layer에서 1st layer의 역할
A.input을 선형 분리 가능하도록 변경

Q.activation func의 필요성
A.layer가 많아도 single layer 형태로 치환되어서

Q.층이 깊어지면 장점
A.문제가 단순해짐

Q.학습의 목표
A.함수를 찾는것

Q.함수를 찾는다는 의미
A.weight를 찾는것

Q.시그모이드 공식
A.1/(1+e^-x)

Q.시그모이드 미분
A.f*(1-f)

Q.목적함수란
A.에러함수

Q.그래디언트 디센트란
A.기울기의 반대 방향으로 weight를 조정해주는 방법

Q.러닝 레이트 역할
A.큰값을 주면 weight이동 많이 작은값을 주면 weight이동 적게

Q.르쿤 방법
A.델타*웨이트를 델타바로 넘겨줌, f'(net)을 곱해줘서 델타로 사용

Q.배니싱 그래디언트란
A.back propagation중 작은 기울기가 곱해져 기울기가 0에 수렴

Q.배니싱 그래디언트 해결방법
A.bottom-up layerwise unsupervisered pre-training, ReLu, 그래디언트 플로우

Q.greedy layer-wise training이란
A.배니싱 그래디언트를 해결하기위해 hidden layer 한 층씩 학습시킴

Q.unsupervised pre-training이란
A.label이 달린 학습데이터가 부족해서, label이 없는 데이터를
greedy layer-wise training으로 사전학습시킴

Q.DBN방식이란
A.vanishing gredient 문제를 해결하기위해
RBM(확률생성) 모형으로 unsupvervised
Greedy layer-wise training을 시킴

Q.Residual net이란
A.기울기가 0이 되지 않게 Gradient Flow 통로를 만들어 둠

Q.output의 형태 2가지
A.one-hot vector, softmax function

Q.인공지능 문제 유형 2가지
A.Regression, classification

Q.Regression ,Binary classification, Multi class classification의
output 활성화 함수와 목적함수
A.
Regression - linear, MSE
Binary classification - sigmoid, binary cross entropy
Multi class classification - softmax, Categorical cross entropy

Q.Regression 언제쓰지
A.출력이 연속값을 갖는 함수를 대상으로
훈련 data를 잘 재현하는 함수를 찾을때 ex)매출액, 생산 속도

Q.Binary classification을 언제쓰지
A.정답이 두 분류로 나뉠때 ex) and or gate

Q.Multi class classification을 언제쓰지
A.정답이 여러 분류일때 ex) mnist(손글씨)

Q.옵티마이저 종류 7가지
A.SGD, learning rate decay, 모멘텀, 네스테로브
아다그래드,RMSP(네스테로브+아다),아담(모멘텀+RMSP)

Q.oneline이란
A.sample 하나마다 update

Q.배치란
A.모든 sample에 대하여 에러를 구하고 weight를 수정

Q.미니 배치란
A.sample들 몇개를 묶어서 에러를 구하고 weight update

Q.sgd 장점 4가지
A.훈련데이터 중복성이 있을때 효율이 좋고 학습빠름,
local minimum에 빠질 위험이 줄어듦,
update 크기가 작은상태로 학습 진행해서 관찰용이
데이터 수집과 최적화가 동시에 진행

Q.SGD와 DGD의 차이점
A.부정확한 방향 빠른 속도, 정확한 방향 느린 속도

Q.learning rate decay란
A.학습이 진행할수록 learning rate를 줄임
(+입력층 크게 출력층 작게)

Q.모멘텀이란 +장점
A.이전 변화량을 더해줘서 가속도를 붙임, 진동방지

Q.nesterov란
A.왔던 만큼 한번 더 간 상태를 가정해서 weight를 수정

Q.아다그래드란 +단점
A.적게 나온 기울기를 중요시 여김
잘 진행해도 변화량이 점점 작아짐

Q.RMSP이란
A.아다+nesterov
네스테로브로 미리 가보고 아다로 적게나온걸 중시하는데
로우를 곱해줘서 현재방향 비율을 남겨둠

Q.아담이란
A.RMSP+모멘텀
모멘텀을 s, RMSP을 r로 정해서 s/r 비율만큼 이동

Q.overfitting 원인
A.전체에 비해 학습 데이터가 너무 적은 경우
generalization이 안좋음

Q.overfitting 해결 3가지
A.Regularization, 드랍아웃, 데이터 확장

Q.Regularization이란
A.목적함수에 complexity penalty를 추가해
낮은 차원처럼 표현

Q.모델 앙상블이란
A.여러가지 모델을 합쳐서 결과에 반영

Q.드랍아웃
A.일부 노드를 없는것처럼 취급해서 학습

Q.드랍아웃 - 앙상블 러닝 
A.모델을 작은 모델로 만들어 각 output을 조합

Q.데이터 확장
A.데이터가 부족해서 여러가지 방법으로 데이터를 늘림